<!-- build time:Sat Jul 26 2025 14:56:27 GMT+0800 (China Standard Time) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.jpg"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><link rel="mask-icon" href="/images/avatar.jpg" color=""><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="7QiG8wJPa5_9Ca0Y8hlhT0rJtNqEekDbao8u-zvq7zA"><meta name="msvalidate.01" content="9A952D9AAB23A5CE98B20FC76E335FA6"><meta name="yandex-verification" content="cb2a7fddf83f1d59"><link rel="alternate" type="application/rss+xml" title="taf.fyi" href="https://taf.fyi/rss.xml"><link rel="alternate" type="application/atom+xml" title="taf.fyi" href="https://taf.fyi/atom.xml"><link rel="alternate" type="application/json" title="taf.fyi" href="https://taf.fyi/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7Cconsolas:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.6"><meta name="keywords" content="DevOps"><link rel="canonical" href="https://taf.fyi/cs/devops/BDplatforms/"><title>大数据平台 All in One 搭建记录 - DevOps - 计算机科学</title><script src="https://unpkg.com/typed.js@2.1.0/dist/typed.umd.js" async></script><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">大数据平台 All in One 搭建记录</h1><div class="meta"><span class="item" title="Created: 2025-05-07 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2025-05-07T00:00:00+08:00">2025-05-07</time> </span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">Symbols count in article</span> <span>11k</span> <span class="text">words</span> </span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">Reading time</span> <span>10 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">taf.fyi</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><img src="/assets/cs/devops/DBplatforms/arch.png"></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/cs/" itemprop="item" rel="index" title="In 计算机科学"><span itemprop="name">计算机科学</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/cs/devops/" itemprop="item" rel="index" title="In DevOps"><span itemprop="name">DevOps</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://taf.fyi/cs/devops/BDplatforms/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="fuuzen"><meta itemprop="description" content="taf.fyi, fuuzen 的个人主页 taf.fyi"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="taf.fyi"></span><div class="body md" itemprop="articleBody"><h2 id="架构介绍"><a class="anchor" href="#架构介绍">#</a> 架构介绍</h2><table><thead><tr><th>Hostname</th><th>IP</th><th>用户</th></tr></thead><tbody><tr><td>controller (跳板机)</td><td>222.200.180.102</td><td>openstack</td></tr><tr><td>cpf-1 (虚拟机)</td><td>10.10.3.183</td><td>ubuntu</td></tr><tr><td>cpf-2 (虚拟机)</td><td>10.10.1.96</td><td>ubuntu</td></tr><tr><td>cpf-3 (虚拟机)</td><td>10.10.3.222</td><td>ubuntu</td></tr><tr><td>cpf-4 (虚拟机)</td><td>10.10.0.176</td><td>ubuntu</td></tr></tbody></table><p>本文在内网环境中的 4 个节点上部署一套包含 Hadoop、Spark、Tez、Flink 的环境，文中提到的跳板机和节点如上所示。</p><p>最终搭建起的架构如图所示：(忽略其中的 zipkin, promethus, grafana 本文暂不讨论)</p><p><img data-src="/assets/cs/devops/DBplatforms/arch.png" alt="架构"></p><h2 id="hadoop"><a class="anchor" href="#hadoop">#</a> Hadoop</h2><p>安装 <span class="exturl" data-url="aHR0cHM6Ly9jd2lraS5hcGFjaGUub3JnL2NvbmZsdWVuY2UvZGlzcGxheS9IQURPT1AvSGFkb29wK0phdmErVmVyc2lvbnM=">兼容版本的 Java</span>，我这里使用 Ubuntu 24.02，直接使用 apt 安装 Java 8 ：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> openjdk-8-jre</pre></td></tr></table></figure><p>通过 <code>which java</code> ，查看软链接，找到 Java 的实际安装目录，在我这里为 <code>/usr/lib/jvm/java-8-openjdk-amd64</code> 。</p><p>下载 <span class="exturl" data-url="aHR0cHM6Ly93d3cuYXBhY2hlLm9yZy9keW4vY2xvc2VyLmNnaS9oYWRvb3AvY29tbW9uLw==">Hadoop</span>，我这里选择下载 <code>hadoop-stable</code> 的为 <code>hadoop-3.4.1</code> 。直接 <code>wget</code> 下载然后 <code>tar -zxvf</code> 解压。我下载时国内网速太慢，挂了梯子下载然后 <code>scp</code> 分发。</p><p>在所有节点上执行如下：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> <span class="token function">tar</span> xzf hadoop-3.4.1.tar.gz <span class="token parameter variable">-C</span> /opt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /opt/hadoop-3.4.1/ /opt/hadoop</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/hadoop</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/hadoop-3.4.1</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token function">sudo</span> <span class="token function">mkdir</span> data</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token function">sudo</span> <span class="token function">mkdir</span> /data/hadoop</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /data/hadoop</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token function">mkdir</span> /data/hadoop/data</pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token function">mkdir</span> /data/hadoop/name</pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token function">mkdir</span> /data/hadoop/pid</pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token function">mkdir</span> /data/hadoop/tmp</pre></td></tr></table></figure><p>然后，在 <code>/opt/hadoop/etc/hadoop</code> 目录下，编辑配置文件：</p><p><code>core-site.xml</code> :</p><figure class="highlight xml"><figcaption data-lang="XML"></figcaption><table><tr><td data-num="1"></td><td><pre># Add these elements:</pre></td></tr><tr><td data-num="2"></td><td><pre>## Do not forget to change the tmp directory</pre></td></tr><tr><td data-num="3"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>fs.defaultFS<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>hdfs://&#123;master&#125;:9000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="6"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="7"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>hadoop.tmp.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/data/hadoop/tmp<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="10"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr></table></figure><p><code>hadoop-env.sh</code> :</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Add these commands:</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token comment">## Do not forget to change the PID directory</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-8-openjdk-amd64</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_PID_DIR</span><span class="token operator">=</span>/data/hadoop/pid</pre></td></tr></table></figure><p><code>hdfs-site.xml</code> :</p><figure class="highlight xml"><figcaption data-lang="XML"></figcaption><table><tr><td data-num="1"></td><td><pre># Add these elements:</pre></td></tr><tr><td data-num="2"></td><td><pre>## dfs.replication property specifies replication factor, you may want to change it. I prefer replication factor of two for small clusters.</pre></td></tr><tr><td data-num="3"></td><td><pre>## Do not forget to change the data directory</pre></td></tr><tr><td data-num="4"></td><td><pre>## Do not forget to change the name directory</pre></td></tr><tr><td data-num="5"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>&#123;master&#125;:9001<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="8"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="9"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.namenode.secondary.http-address<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>&#123;master&#125;:9002<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="12"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="13"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.webhdfs.enabled<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>true<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="16"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="17"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.replication<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="19"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="20"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="21"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.data.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/data/hadoop/data<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="24"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="25"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="26"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.name.dir<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="27"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>/data/hadoop/name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="28"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="29"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="30"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>dfs.permission<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="31"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>false<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="32"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr></table></figure><p><code>mapred-site.xml</code> :</p><figure class="highlight xml"><figcaption data-lang="XML"></figcaption><table><tr><td data-num="1"></td><td><pre># Add these elements:</pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.framework.name<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>yarn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="5"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="6"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="7"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapreduce.application.classpath<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="8"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="9"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="10"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>mapred.job.tracker<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>&#123;master&#125;:54311<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="13"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="14"></td><td><pre># The host and port that the MapReduce job tracker runs at.  If "local", then jobs are run in-process as a single map and reduce task.</pre></td></tr></table></figure><p><code>workers</code> :</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Add the names of all workers</span></pre></td></tr><tr><td data-num="2"></td><td><pre>cpf-2</pre></td></tr><tr><td data-num="3"></td><td><pre>cpf-3</pre></td></tr><tr><td data-num="4"></td><td><pre>cpf-4</pre></td></tr></table></figure><p><code>yarn-site.xml</code> :</p><figure class="highlight xml"><figcaption data-lang="XML"></figcaption><table><tr><td data-num="1"></td><td><pre># Add these lines</pre></td></tr><tr><td data-num="2"></td><td><pre>## You may want to change the max-disk-utilization-per-disk-percentage.</pre></td></tr><tr><td data-num="3"></td><td><pre>## Notice that the value of "yarn.nodemanager.hostname" property is &#123;worker&#125;. We leave it for now, and will change it later</pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.aux-services<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>mapreduce_shuffle<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="7"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="8"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.env-whitelist<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="11"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="12"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="14"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>95<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="15"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="16"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="17"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.resourcemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="18"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>&#123;master&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="19"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="20"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>yarn.nodemanager.hostname<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>&#123;worker&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="23"></td><td><pre>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr></table></figure><p>通过 <code>sudo vim /etc/profile.d/apache-01-hadoop.sh</code> 编辑一个 Hadoop 相关环境变量的配置文件，这个文件命名是为了在后续加载 Spark、Tez 环境变量的配置文件之前完成加载，因为后者依赖于这里的一些环境变量。</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># Java</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-8-openjdk-amd64</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment"># Haddop &amp; YARN</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">PDSH_RCMD_TYPE</span><span class="token operator">=</span>ssh</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CONF_DIR</span><span class="token operator">=</span>/opt/hadoop/etc/hadoop</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_MAPRED_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_COMMON_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HDFS_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="10"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_YARN_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="11"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/bin:<span class="token environment constant">$PATH</span></pre></td></tr><tr><td data-num="12"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_NAMENODE_USER</span><span class="token operator">=</span><span class="token string">"ubuntu"</span></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_DATANODE_USER</span><span class="token operator">=</span><span class="token string">"ubuntu"</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_SECONDARYNAMENODE_USER</span><span class="token operator">=</span><span class="token string">"ubuntu"</span></pre></td></tr><tr><td data-num="15"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_RESOURCEMANAGER_USER</span><span class="token operator">=</span><span class="token string">"ubuntu"</span></pre></td></tr><tr><td data-num="16"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_NODEMANAGER_USER</span><span class="token operator">=</span><span class="token string">"ubuntu"</span></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/lib/native</pre></td></tr></table></figure><p>然后就可以启动 hadoop 试试看了，常用操作如下：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 格式化 (清除) hdfs</span></pre></td></tr><tr><td data-num="2"></td><td><pre>hadoop namenode <span class="token parameter variable">-format</span></pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># 启动 hdfs 和 yarn 集群</span></pre></td></tr><tr><td data-num="5"></td><td><pre>/opt/hadoop/sbin/start-dfs.sh</pre></td></tr><tr><td data-num="6"></td><td><pre>/opt/hadoop/sbin/start-yarn.sh</pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token comment"># 停止 hdfs 和 yarn 集群</span></pre></td></tr><tr><td data-num="9"></td><td><pre>/opt/hadoop/sbin/stop-yarn.sh</pre></td></tr><tr><td data-num="10"></td><td><pre>/opt/hadoop/sbin/stop-dfs.sh</pre></td></tr></table></figure><p>万一误操作导致 hdfs 文件系统爆了，在所有节点上执行如下脚本：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token shebang important">#!/bin/bash</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">rm</span> <span class="token parameter variable">-rf</span> /data/hadoop/data</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">rm</span> <span class="token parameter variable">-rf</span> /data/hadoop/name</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">rm</span> <span class="token parameter variable">-rf</span> /data/hadoop/pid</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token function">rm</span> <span class="token parameter variable">-rf</span> /data/hadoop/tmp</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /data/hadoop/data</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /data/hadoop/name</pre></td></tr><tr><td data-num="8"></td><td><pre><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /data/hadoop/pid</pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /data/hadoop/tmp</pre></td></tr></table></figure><h2 id="spark"><a class="anchor" href="#spark">#</a> Spark</h2><p>同样首先还是在所有节点上安装二进制程序到 <code>/opt</code> 下：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> <span class="token function">tar</span> xzf spark-3.4.4-bin-hadoop3.tgz <span class="token parameter variable">-C</span> /opt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /opt/spark-3.4.4-bin-hadoop3/ /opt/spark</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/spark</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/spark-3.4.4-bin-hadoop3</pre></td></tr></table></figure><p>然后将 hadoop 的相关配置文件复制到 spark 的配置目录下：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">cp</span> /opt/hadoop/etc/hadoop/core-site.xml /opt/spark/conf/</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">cp</span> /opt/hadoop/etc/hadoop/hdfs-site.xml /opt/spark/conf/</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">cp</span> /opt/hadoop/etc/hadoop/yarn-site.xml /opt/spark/conf/</pre></td></tr></table></figure><p>然后，在 <code>/opt/spark/conf</code> 目录下，编辑配置文件：</p><p><code>workers</code> :</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 写入 worker 节点 hostname (当然不一定要复制模板)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>cpf-2</pre></td></tr><tr><td data-num="3"></td><td><pre>cpf-3</pre></td></tr><tr><td data-num="4"></td><td><pre>cpf-4</pre></td></tr></table></figure><p><code>cp spark-env.sh.template spark-env.sh</code> 后在 <code>spark-env.sh</code> 增加如下内容 :</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-8-openjdk-amd64</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_HOME</span><span class="token operator">=</span>/opt/spark</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$SPARK_HOME</span>/conf</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/opt/hadoop</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/etc/hadoop</pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$HADOOP_HOME</span>/etc/hadoop</pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_WORKER_DIR</span><span class="token operator">=</span>/tmp/spark/work</pre></td></tr></table></figure><p><code>cp log4j2.properties.template log4j2.properties</code> 使用默认配置即可。</p><p><code>sudo vim /etc/profile.d/apache-02-spark.sh</code> 配置 Spark 相关环境变量：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_HOME</span><span class="token operator">=</span>/opt/spark</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">SPARK_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$SPARK_HOME</span>/conf</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$SPARK_HOME</span>/bin:<span class="token environment constant">$PATH</span></pre></td></tr></table></figure><p>最后上传 Spark 相关 jar 文件到 HDFS：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>hdfs dfs <span class="token parameter variable">-mkdir</span> <span class="token parameter variable">-p</span> /user/spark/share/lib</pre></td></tr><tr><td data-num="2"></td><td><pre>hadoop fs <span class="token parameter variable">-put</span> /opt/spark/jars/* /user/spark/share/lib/</pre></td></tr></table></figure><h2 id="tez"><a class="anchor" href="#tez">#</a> Tez</h2><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> <span class="token function">tar</span> xzf apache-tez-0.10.5-bin.tar.gz <span class="token parameter variable">-C</span> /opt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /opt/apache-tez-0.10.5-bin/ /opt/tez</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/tez</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/apache-tez-0.10.5-bin</pre></td></tr></table></figure><p>将解压出来的 <code>share/tez.tar.gz</code> 上传到 hdfs 中：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>hadoop fs <span class="token parameter variable">-mkdir</span> <span class="token parameter variable">-p</span> /apps/tez</pre></td></tr><tr><td data-num="2"></td><td><pre>hadoop fs <span class="token parameter variable">-copyFromLocal</span> /opt/tez/share/tez.tar.gz /apps/tez/</pre></td></tr></table></figure><p>准备一个配置文件 <code>tez-site.xml</code> ，在 <code>/opt/tez/conf</code> 目录中将有自动生成的配置文件模板，所以我们的配置文件也放在这里。这里仅配置一项即可：</p><figure class="highlight xml"><figcaption data-lang="XML"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>configuration</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="2"></td><td><pre>	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="3"></td><td><pre>		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>tez.lib.uris<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="4"></td><td><pre>		<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>$&#123;fs.defaultFS&#125;/apps/tez/tez.tar.gz<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="5"></td><td><pre>	<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">></span></span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>configuration</span><span class="token punctuation">></span></span></pre></td></tr></table></figure><p><code>sudo vim /etc/profile.d/apache-03-tez.sh</code> 配置 Spark 相关环境变量：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">TEZ_HOME</span><span class="token operator">=</span>/opt/tez</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">TEZ_CONF_DIR</span><span class="token operator">=</span><span class="token variable">$&#123;TEZ_HOME&#125;</span>/conf</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">TEZ_JARS</span><span class="token operator">=</span><span class="token variable">$&#123;TEZ_HOME&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_CLASSPATH</span><span class="token operator">=</span><span class="token variable">$&#123;HADOOP_CLASSPATH&#125;</span><span class="token builtin class-name">:</span><span class="token variable">$&#123;TEZ_CONF_DIR&#125;</span><span class="token builtin class-name">:</span><span class="token variable">$&#123;TEZ_JARS&#125;</span>/*:<span class="token variable">$&#123;TEZ_JARS&#125;</span>/lib/*</pre></td></tr></table></figure><h2 id="flink"><a class="anchor" href="#flink">#</a> Flink</h2><p>Flink on YARN 支持如下几种模式：</p><ul><li>Session 模式，先将 Flink 集群作为一个作业提交给 YARN 来启动集群，然后再提交作业，且后续提交的作业共享 Flink 集群的资源。<ul><li>Attached 模式，顾名思义前台运行；</li><li>Dettached 模式，顾名思义后台运行；</li></ul></li><li>Application 模式，直接提交作业。提交后在 YARN 上启动一个 Flink 集群，应用程序 jar 的 <code>main()</code> 方法将在 YARN 中的 JobManager 上执行。应用程序一完成，集群就会关闭。</li></ul><p>接下来部署一个 Dettached Session 模式的 Flink 集群，这将同时提供一个 Web UI 供提交监控 Flink 作业。</p><p>首先还是在所有节点上安装二进制程序到 <code>/opt</code> 下：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> <span class="token function">tar</span> xzf flink-2.0.0.tar.gz <span class="token parameter variable">-C</span> /opt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /opt/flink-2.0.0/ /opt/flink</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/flink</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token function">sudo</span> <span class="token function">chown</span> <span class="token parameter variable">-R</span> ubuntu:ubuntu /opt/flink-2.0.0</pre></td></tr></table></figure><p>然后修改配置文件 <code>/opt/flink/conf/config.yaml</code> ，指定 Flink 版本所兼容的 Java 版本，例如这里 Flink 2.0.0 需要 Java 17+ 版本，我将其指定为 apt 安装的 Java 17 安装路径：</p><figure class="highlight yaml"><figcaption data-lang="YAML"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token key atrule">env</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token key atrule">java</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token key atrule">home</span><span class="token punctuation">:</span> /usr/lib/jvm/java<span class="token punctuation">-</span>17<span class="token punctuation">-</span>openjdk<span class="token punctuation">-</span>amd64</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment">#</span></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token comment"># ......</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token comment">#</span></pre></td></tr><tr><td data-num="7"></td><td><pre><span class="token key atrule">rest</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="8"></td><td><pre>  <span class="token key atrule">port</span><span class="token punctuation">:</span> xxxxx <span class="token comment"># 绑定 Web UI 到指定端口</span></pre></td></tr></table></figure><p>上面配置修改了 <code>rest.port</code> 绑定 Web UI 端口，因为默认的 8081 端口已经被 Hadoop 和 YARN 占用了，避免每次启动都是随机端口。尽管如此，还是无法绑定到指定 address 上，因为提交 Flink 集群后，前端分配到哪个节点是不确定的，只能在启动后确定。</p><p>但是，我们的 Hadoop 和 YARN 依然依赖于 Java 8 所以当前环境变量 <code>JAVA_HOME</code> 指向的 Java 路径是 Java 8 的，并且 Flink 也依赖于这个环境变量！所以即使加了上面的配置启动集群时还是会有 Java 版本不兼容的报错。所以只好启动的时候指定一个高版本 Java 路径：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-17-openjdk-amd64 ./bin/yarn-session.sh <span class="token parameter variable">-d</span></pre></td></tr></table></figure><p>然后会输出 Web UI 的访问方式，以及 Flink 集群的 application id 用于停止集群。当然，YARN 面板上随时可以查到这个 Flink 集群的 application id，所以并不一定需要记下它。</p><p>当然，停止集群的时候也要指定 Java 路径：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin class-name">echo</span> <span class="token string">"stop"</span> <span class="token operator">|</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-17-openjdk-amd64 ./bin/yarn-session.sh <span class="token parameter variable">-id</span> <span class="token operator">&lt;</span>application_id<span class="token operator">></span></pre></td></tr></table></figure><p>但是接下来还有可能遇到一个问题导致提交的作业永远无法顺利执行，就是资源限制问题。和 Spark 和 Tez 不太一样，Flink 需要一个作业作为 Flink 集群，若我们使用同一个用户启动 Flink 集群，再提交其他非 Flink 作业时可能会发现作业卡住永远没有进展，是因为当前用户 AM 资源上限被 Flink 集群这个作业占完了。似乎能修改配置解决，有比较好的解决方案了后续再补上...</p><h2 id="访问内网环境"><a class="anchor" href="#访问内网环境">#</a> 访问内网环境</h2><p>在 <code>~/.ssh/config</code> 中添加如下配置：</p><pre><code class="language-conf">Host hadoop-tunnel
  HostName 222.200.180.102
  User openstack
  # HDFS Web UI (namenode)
  LocalForward 9870 10.10.3.183:9870
  # HDFS Web UI (secondary namenode)
  LocalForward 9868 10.10.3.183:9868
  # YARN Web UI
  LocalForward 8088 10.10.3.183:8088
  LocalForward 8042 10.10.1.96:8042
  LocalForward 8043 10.10.3.222:8042
  LocalForward 8044 10.10.0.176:8042
  # zipkin Web UI
  LocalForward 9411 10.10.3.183:9411
  # JobHistory Web UI
  LocalForward 19888 10.10.3.183:19888
  # Promethues Web UI
  LocalForward 9090 10.10.3.183:9090
  # Node_Exporter Web UI
  LocalForward 9100 10.10.3.183:9100
  LocalForward 9200 10.10.1.96:9100
  LocalForward 9300 10.10.3.222:9100
  LocalForward 9400 10.10.0.176:9100
  # Grafana Web UI 监控可视化
  LocalForward 3000 10.10.3.183:3000

Host cpf-1
  HostName 10.10.3.183
  User ubuntu
  ProxyCommand ssh -W %h:%p hadoop-tunnel

Host cpf-2
  HostName 10.10.1.96
  User ubuntu
  ProxyCommand ssh -W %h:%p hadoop-tunnel

Host cpf-3
  HostName 10.10.3.222
  User ubuntu
  ProxyCommand ssh -W %h:%p hadoop-tunnel

Host cpf-4
  HostName 10.10.0.176
  User ubuntu
  ProxyCommand ssh -W %h:%p hadoop-tunnel
</code></pre><p>基于上述配置：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">ssh</span> <span class="token parameter variable">-N</span> hadoop-tunnel  <span class="token comment"># 会挂着转发全部 Web 应用</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">ssh</span> hadoop-tunnel     <span class="token comment"># 登录跳板机，顺便转发全部 Web 应用</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">ssh</span> cpf-1             <span class="token comment"># 登录虚拟机</span></pre></td></tr></table></figure><h2 id="提交任务"><a class="anchor" href="#提交任务">#</a> 提交任务</h2><p>除了 Flink 都需要连接到虚拟机上提交。</p><h3 id="spark-2"><a class="anchor" href="#spark-2">#</a> Spark</h3><p>需要通过 <code>spark-submit</code> 来提交。</p><p>由于是 Spark on YARN 的部署方式，提交的任务中，SparkSession 需要增加一项配置 <code>.config(&quot;spark.hadoop.fs.defaultFS&quot;, &quot;hdfs://cpf-1:9000&quot;)</code> 来指定使用 YARN 和 hadoop，并且在提交时指定 <code>--master yarn --deploy-mode cluster</code> 。</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>spark-submit</pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token parameter variable">--master</span> <span class="token function">yarn</span> <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  --deploy-mode cluster <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  xxxApp</pre></td></tr></table></figure><p>使用 Spot 追踪，假设追踪后端为 zipkin 并且部署在 <span class="exturl" data-url="aHR0cDovL2NwZi0xOjk0MTE=">http://cpf-1:9411</span>，需要增加：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>spark-submit</pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token parameter variable">--jars</span> spot-complete-3.5_2.12-0.1.1.jar <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.extraListeners</span><span class="token operator">=</span>com.xebia.data.spot.TelemetrySparkListener <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.otel.traces.exporter</span><span class="token operator">=</span>zipkin <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="5"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.otel.exporter.zipkin.endpoint</span><span class="token operator">=</span>http://cpf-1:9411/api/v2/spans <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="6"></td><td><pre>  xxxApp</pre></td></tr></table></figure><p>这里的 <code>spot-complete-3.5_2.12-x.x.x.jar</code> 从 这个 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z1dXplbi9zcG90">fork</span> 的 CI (build) 下载，因为原项目仓库依赖缺少 opentelemetry-exporter-zipkin 会导致用 zipkin 测试失败。</p><p>python 测试为例如下：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>spark <span class="token operator">=</span> SparkSession\</pre></td></tr><tr><td data-num="4"></td><td><pre>        <span class="token punctuation">.</span>builder\</pre></td></tr><tr><td data-num="5"></td><td><pre>        <span class="token punctuation">.</span>config<span class="token punctuation">(</span><span class="token string">"spark.hadoop.fs.defaultFS"</span><span class="token punctuation">,</span> <span class="token string">"hdfs://cpf-1:9000"</span><span class="token punctuation">)</span>\</pre></td></tr><tr><td data-num="6"></td><td><pre>        <span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">"test01"</span><span class="token punctuation">)</span>\</pre></td></tr><tr><td data-num="7"></td><td><pre>        <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_python_version</span><span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="10"></td><td><pre>  <span class="token keyword">import</span> sys</pre></td></tr><tr><td data-num="11"></td><td><pre>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Python version:"</span><span class="token punctuation">,</span> sys<span class="token punctuation">.</span>version<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token keyword">def</span> <span class="token function">get_packages</span><span class="token punctuation">(</span>_<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>  <span class="token keyword">import</span> pkg_resources</pre></td></tr><tr><td data-num="15"></td><td><pre>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>pkg<span class="token punctuation">.</span>key<span class="token punctuation">:</span> pkg<span class="token punctuation">.</span>version <span class="token keyword">for</span> pkg <span class="token keyword">in</span> pkg_resources<span class="token punctuation">.</span>working_set<span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre><span class="token comment"># Driver 端的 Python 版本</span></pre></td></tr><tr><td data-num="18"></td><td><pre>get_python_version<span class="token punctuation">(</span><span class="token number">114</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre></pre></td></tr><tr><td data-num="20"></td><td><pre><span class="token comment"># Executor 端的 Python 版本</span></pre></td></tr><tr><td data-num="21"></td><td><pre>spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>get_python_version<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre><span class="token comment"># 列出所有 Driver 端的包名</span></pre></td></tr><tr><td data-num="24"></td><td><pre>get_packages<span class="token punctuation">(</span><span class="token number">514</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre></pre></td></tr><tr><td data-num="26"></td><td><pre><span class="token comment"># 列出所有 Executor 端的包名</span></pre></td></tr><tr><td data-num="27"></td><td><pre>spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>get_packages<span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre></pre></td></tr><tr><td data-num="29"></td><td><pre>spark<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>提交测试并追踪：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>spark-submit <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token parameter variable">--master</span> <span class="token function">yarn</span> <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  --deploy-mode cluster <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  <span class="token parameter variable">--jars</span> spot-complete-3.5_2.12-0.1.1.jar <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="5"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.extraListeners</span><span class="token operator">=</span>com.xebia.data.spot.TelemetrySparkListener <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="6"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.otel.traces.exporter</span><span class="token operator">=</span>zipkin <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="7"></td><td><pre>  <span class="token parameter variable">--conf</span> <span class="token assign-left variable">spark.otel.exporter.zipkin.endpoint</span><span class="token operator">=</span>http://cpf-1:9411/api/v2/spans <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="8"></td><td><pre>  test.py</pre></td></tr></table></figure><h3 id="flink-2"><a class="anchor" href="#flink-2">#</a> Flink</h3><p>可以直接通过 Web UI 提交，也可以登陆虚拟机通过指令提交，只是提交时必须指定 <code>JAVA_HOME</code> （默认的 <code>JAVA_HOME</code> 是给 hadoop 和 yarn 用的 java 8）：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/usr/lib/jvm/java-17-openjdk-amd64 <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  /opt/flink/bin/flink run <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  /opt/flink/examples/streaming/TopSpeedWindowing.jar</pre></td></tr></table></figure><h3 id="tez-2"><a class="anchor" href="#tez-2">#</a> Tez</h3><p>虚拟机上通过指令按照普通的 MapReduce 任务提交即可，例如：</p><figure class="highlight shell"><figcaption data-lang="Bash"></figcaption><table><tr><td data-num="1"></td><td><pre>hadoop <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  jar tez-examples-0.10.5.jar <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  orderedwordcount <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="4"></td><td><pre>  /tmp/test/wordcount-input <span class="token punctuation">\</span></pre></td></tr><tr><td data-num="5"></td><td><pre>  /tmp/test/wordcount-output</pre></td></tr></table></figure><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"><div class="tags"><a href="/tags/DevOps/" rel="tag"><i class="ic i-tag"></i> DevOps</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2025-07-26 14:50:55" itemprop="dateModified" datetime="2025-07-26T14:50:55+08:00">2025-07-26</time> </span><span id="cs/devops/BDplatforms/" data-flag-title="大数据平台 All in One 搭建记录" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text" id="busuanzi_container_page_pv">Views <span id="busuanzi_value_page_pv"></span> times</span></span></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>fuuzen</li><li class="link"><strong>Post link: </strong><a href="https://taf.fyi/cs/devops/BDplatforms/" title="大数据平台 All in One 搭建记录">https://taf.fyi/cs/devops/BDplatforms/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">架构介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop"><span class="toc-number">2.</span> <span class="toc-text">Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark"><span class="toc-number">3.</span> <span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tez"><span class="toc-number">4.</span> <span class="toc-text">Tez</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#flink"><span class="toc-number">5.</span> <span class="toc-text">Flink</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E7%8E%AF%E5%A2%83"><span class="toc-number">6.</span> <span class="toc-text">访问内网环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="toc-number">7.</span> <span class="toc-text">提交任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-2"><span class="toc-number">7.1.</span> <span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flink-2"><span class="toc-number">7.2.</span> <span class="toc-text">Flink</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tez-2"><span class="toc-number">7.3.</span> <span class="toc-text">Tez</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li><a href="/cs/devops/acme/" rel="bookmark" title="acme.sh插件">acme.sh插件</a></li><li><a href="/cs/devops/overleaf/" rel="bookmark" title="OverLeaf自建记录">OverLeaf自建记录</a></li><li class="active"><a href="/cs/devops/BDplatforms/" rel="bookmark" title="大数据平台 All in One 搭建记录">大数据平台 All in One 搭建记录</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="fuuzen" data-src="/images/avatar.jpg"><p class="name" itemprop="name">fuuzen</p><div class="description" itemprop="description">fuuzen 的个人主页 taf.fyi</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">23</span> <span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">4</span> <span class="name">tags</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2Z1dXplbg==" title="https:&#x2F;&#x2F;github.com&#x2F;fuuzen"><i class="ic i-github"></i></span> <span class="exturl item telegram" data-url="aHR0cHM6Ly90Lm1lL2Z1dXNlbl9ub19wYXJ0eQ==" title="https:&#x2F;&#x2F;t.me&#x2F;fuusen_no_party"><i class="ic i-paper-plane"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTE3NzMxNDU3NTU=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;1773145755"><i class="ic i-cloud-music"></i></span> <span class="exturl item bilibili" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vNTE0OTI3MjkxLw==" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;514927291&#x2F;"><i class="ic i-bilibili"></i></span> <a href="/rss.xml" title="&#x2F;rss.xml" class="item rss"><i class="ic i-rss"></i></a></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>About</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>Posts</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>Archives</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>Categories</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>Tags</a></li></ul></li><li class="item"><a href="/notebook/" rel="section"><i class="ic i-pen"></i>Notebook</a></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>Friends</a></li><li class="item"><a href="/acgn/" rel="section"><i class="ic i-magic"></i>ACGN</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/cs/db/mini-oceanbase/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/acgn/eustia/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"></div><div class="status"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">fuuzen @ taf.fyi</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-eye"></i> </span><span class="text" id="busuanzi_container_site_pv"><span id="busuanzi_value_site_pv"></span> views </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-heart"></i> </span><span class="text" id="busuanzi_container_site_uv"><span id="busuanzi_value_site_uv"></span> visitors </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="Symbols count total">168k words</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="Reading time total">2:32</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"cs/devops/BDplatforms/",favicon:{show:"Ciallo～(∠・ω< )⌒★!",hide:"Signaled Out"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},copy_tex:!0,katex:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="//fastly.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.6"></script></body></html><!-- rebuild by hrmmi -->